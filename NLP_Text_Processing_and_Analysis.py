# -*- coding: utf-8 -*-
"""NLP ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HpL16JD4Hy1pZlielsAky-kEb93irPv
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("tmehul/spamcsv")

print("Path to dataset files:", path)

import pandas as pd

# Load the dataset
df = pd.read_csv(path + "/spam.csv", encoding="latin1")  # Adjust encoding if needed
df = df[['v1', 'v2']]  # Selecting the relevant columns if necessary
df.columns = ['label', 'message']  # Renaming for clarity

print(df.head())

# Number of SMS messages
num_messages = len(df)

# Number of spam messages
num_spam = len(df[df['label'] == 'spam'])

# Word count of all messages
df['word_count'] = df['message'].apply(lambda x: len(x.split()))

# Average number of words per message
avg_words = df['word_count'].mean()

# Finding the most frequent words
from collections import Counter

word_list = " ".join(df['message']).split()
word_freq = Counter(word_list)

most_common_words = word_freq.most_common(5)
rare_words = sum(1 for word, count in word_freq.items() if count == 1)

# Print results
print(f"Total messages: {num_messages}")
print(f"Spam messages: {num_spam}")
print(f"Average words per message: {avg_words:.2f}")
print(f"5 Most frequent words: {most_common_words}")
print(f"Number of rare words (appear once): {rare_words}")

import nltk

# Download the necessary resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')  # Sometimes useful for lemmatization

# Then proceed with your code

import nltk

# Delete the cache and reinstall punkt
nltk.data.path.append('/usr/local/share/nltk_data')  # Ensure correct path
nltk.download('punkt', force=True)

import os
import nltk

# Set environment variable
os.environ["NLTK_DATA"] = "/usr/local/share/nltk_data"

# Then try downloading again
nltk.download('punkt', force=True)

import nltk
import shutil

# Remove previous NLTK data folder (Colab stores it under /root/nltk_data)
shutil.rmtree('/root/nltk_data', ignore_errors=True)

# Force fresh installation of necessary packages
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer, PorterStemmer

# Ensure necessary downloads
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Example processing
text = str(df['message'][0])  # Ensure it's a string

# Use RegexpTokenizer instead of word_tokenize()
tokenizer = RegexpTokenizer(r'\w+')  # This extracts only words, avoiding the punkt issue
tokens_nltk = tokenizer.tokenize(text)

lemmas_nltk = [lemmatizer.lemmatize(token) for token in tokens_nltk]
stems_nltk = [stemmer.stem(token) for token in tokens_nltk]

print("NLTK Tokenization (using RegexpTokenizer):", tokens_nltk)
print("NLTK Lemmatization:", lemmas_nltk)
print("NLTK Stemming:", stems_nltk)

import spacy

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

# Example processing
text = str(df['message'][0])  # Ensure it's a string
doc = nlp(text)

# Extract tokenization, lemmatization, and stemming (in SpaCy, lemmatization is used instead of stemming)
tokens_spacy = [token.text for token in doc]
lemmas_spacy = [token.lemma_ for token in doc]
stems_spacy = [token.lemma_ for token in doc]  # SpaCy doesn't have separate stemming, so we use lemma_

print("SpaCy Tokenization:", tokens_spacy)
print("SpaCy Lemmatization:", lemmas_spacy)
print("SpaCy 'Stemming' (same as Lemmatization in SpaCy):", stems_spacy)

from collections import Counter

# Create new columns in dataframe to store processed versions
df['tokens_nltk'] = df['message'].apply(lambda x: RegexpTokenizer(r'\w+').tokenize(str(x)))
df['lemmas_nltk'] = df['tokens_nltk'].apply(lambda tokens: [WordNetLemmatizer().lemmatize(token) for token in tokens])
df['stems_nltk'] = df['tokens_nltk'].apply(lambda tokens: [PorterStemmer().stem(token) for token in tokens])

# Repeat for SpaCy
import spacy

nlp = spacy.load("en_core_web_sm")

def spacy_processing(text):
    doc = nlp(str(text))
    tokens = [token.text for token in doc]
    lemmas = [token.lemma_ for token in doc]
    stems = lemmas  # Stemming in SpaCy is handled by lemmas
    return tokens, lemmas, stems

df[['tokens_spacy', 'lemmas_spacy', 'stems_spacy']] = df['message'].apply(lambda x: pd.Series(spacy_processing(x)))

# Compute Statistics for Each Version
def compute_stats(column):
    word_list = [word for message in df[column] for word in message]  # Flatten list
    word_freq = Counter(word_list)

    num_messages = len(df)
    num_spam = len(df[df['label'] == 'spam'])
    avg_words = sum(len(message) for message in df[column]) / num_messages
    most_common_words = word_freq.most_common(5)
    rare_words = sum(1 for word, count in word_freq.items() if count == 1)

    return num_messages, num_spam, avg_words, most_common_words, rare_words

# Print stats for tokenized, lemmatized, and stemmed versions
for version in ['tokens_nltk', 'lemmas_nltk', 'stems_nltk', 'tokens_spacy', 'lemmas_spacy', 'stems_spacy']:
    stats = compute_stats(version)
    print(f"\nStatistics for {version}:")
    print(f"Total messages: {stats[0]}")
    print(f"Spam messages: {stats[1]}")
    print(f"Average words per message: {stats[2]:.2f}")
    print(f"5 Most frequent words: {stats[3]}")
    print(f"Number of rare words (appear once): {stats[4]}")

import requests
from bs4 import BeautifulSoup

# Wikipedia page URL
url = "https://en.wikipedia.org/wiki/Duck"
response = requests.get(url)

# Parse HTML content
soup = BeautifulSoup(response.text, "html.parser")

# Extract page title
title = soup.find("h1").text
print(f"Page Title: {title}")

# Extract all headings (subsections)
headings = [h.text for h in soup.find_all(["h2", "h3"])]
print("\nHeadings Found:")
print("\n".join(headings))

# Extract all paragraph text
paragraphs = [p.text for p in soup.find_all("p")]

# Print first 5 paragraphs
print("\nSample Paragraphs:")
for i in range(5):
    print(f"\nParagraph {i+1}:\n{paragraphs[i]}")

import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer, PorterStemmer

# Ensure necessary downloads
nltk.download('wordnet')

# Initialize NLTK tools
tokenizer = RegexpTokenizer(r'\w+')
lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()

# Example processing for scraped text
def process_text_nltk(text):
    tokens = tokenizer.tokenize(text)
    lemmas = [lemmatizer.lemmatize(token) for token in tokens]
    stems = [stemmer.stem(token) for token in tokens]
    return tokens, lemmas, stems

# Assuming 'paragraphs' contains the scraped Wikipedia text
processed_nltk = [process_text_nltk(paragraph) for paragraph in paragraphs]

# Display sample results
for i in range(3):  # Print first 3 processed paragraphs
    print(f"\nParagraph {i+1} (NLTK):")
    print(f"Tokens: {processed_nltk[i][0]}")
    print(f"Lemmas: {processed_nltk[i][1]}")
    print(f"Stems: {processed_nltk[i][2]}")

import spacy

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

# Example processing for scraped text
def process_text_spacy(text):
    doc = nlp(text)
    tokens = [token.text for token in doc]
    lemmas = [token.lemma_ for token in doc]
    stems = lemmas  # Since SpaCy uses lemmatization instead of traditional stemming
    return tokens, lemmas, stems

# Assuming 'paragraphs' contains the scraped Wikipedia text
processed_spacy = [process_text_spacy(paragraph) for paragraph in paragraphs]

# Display sample results
for i in range(3):  # Print first 3 processed paragraphs
    print(f"\nParagraph {i+1} (SpaCy):")
    print(f"Tokens: {processed_spacy[i][0]}")
    print(f"Lemmas: {processed_spacy[i][1]}")
    print(f"Stems (same as Lemmas in SpaCy): {processed_spacy[i][2]}")

from google.colab import files

# Upload the file manually from local machine
uploaded = files.upload()

# Load text from file (assuming filename is 'WhatsApp Chat.txt')
file_path = "WhatsApp Chat.txt"
with open(file_path, "r", encoding="utf-8") as file:
    chat_text = file.read()

print("Sample WhatsApp chat text:\n", chat_text[:500])  # Display first 500 characters

import nltk
from nltk.tokenize import RegexpTokenizer

# Ensure necessary downloads
nltk.download('punkt')

# Initialize tokenization
tokenizer = RegexpTokenizer(r'\w+')

# Hebrew stemming function (basic heuristic approach)
def simple_hebrew_stemmer(word):
    suffixes = ["ים", "ות", "ה", "ים", "ות", "י", "ת"]
    for suffix in suffixes:
        if word.endswith(suffix):
            return word[:-len(suffix)]
    return word

# Processing function
def process_text_nltk(text):
    tokens = tokenizer.tokenize(text)
    stems = [simple_hebrew_stemmer(token) for token in tokens]  # Apply heuristic stemming
    return tokens, stems  # No official NLTK lemmatization for Hebrew

tokens_nltk, stems_nltk = process_text_nltk(chat_text)

print("\nNLTK Tokenization (Hebrew):", tokens_nltk[:50])
print("NLTK Stemming (Hebrew - heuristic):", stems_nltk[:50])

import spacy

# Load a blank Hebrew tokenizer (since we can't install the full model)
nlp = spacy.blank("he")  # Creates a SpaCy pipeline for Hebrew without pre-trained components

# Processing function for tokenization only (lemmatization won't work without a full model)
def process_text_spacy(text):
    doc = nlp(text)
    tokens = [token.text for token in doc]
    lemmas = tokens  # Since no Hebrew lemmatizer is available, we'll keep tokens as lemmas
    stems = tokens   # Stemming also isn't supported in SpaCy for Hebrew, so we use tokens

    return tokens, lemmas, stems

# Process the Hebrew WhatsApp chat
tokens_spacy, lemmas_spacy, stems_spacy = process_text_spacy(chat_text)

# Display results
print("\nSpaCy Tokenization (Hebrew):", tokens_spacy[:50])
print("SpaCy Lemmatization (Fallback to Tokens):", lemmas_spacy[:50])
print("SpaCy 'Stemming' (Fallback to Tokens):", stems_spacy[:50])