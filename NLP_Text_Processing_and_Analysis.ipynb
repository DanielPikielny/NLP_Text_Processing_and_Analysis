{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGCUq0go1vW9",
        "outputId": "b450233c-3ecb-419e-9adf-e00243492ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/spamcsv\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"tmehul/spamcsv\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(path + \"/spam.csv\", encoding=\"latin1\")  # Adjust encoding if needed\n",
        "df = df[['v1', 'v2']]  # Selecting the relevant columns if necessary\n",
        "df.columns = ['label', 'message']  # Renaming for clarity\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB0Wf7UQ1-RX",
        "outputId": "320c8f8b-944c-47a6-8eea-ca64f7c94497"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of SMS messages\n",
        "num_messages = len(df)\n",
        "\n",
        "# Number of spam messages\n",
        "num_spam = len(df[df['label'] == 'spam'])\n",
        "\n",
        "# Word count of all messages\n",
        "df['word_count'] = df['message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Average number of words per message\n",
        "avg_words = df['word_count'].mean()\n",
        "\n",
        "# Finding the most frequent words\n",
        "from collections import Counter\n",
        "\n",
        "word_list = \" \".join(df['message']).split()\n",
        "word_freq = Counter(word_list)\n",
        "\n",
        "most_common_words = word_freq.most_common(5)\n",
        "rare_words = sum(1 for word, count in word_freq.items() if count == 1)\n",
        "\n",
        "# Print results\n",
        "print(f\"Total messages: {num_messages}\")\n",
        "print(f\"Spam messages: {num_spam}\")\n",
        "print(f\"Average words per message: {avg_words:.2f}\")\n",
        "print(f\"5 Most frequent words: {most_common_words}\")\n",
        "print(f\"Number of rare words (appear once): {rare_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWs0r-_I2DYR",
        "outputId": "5a7462f6-f9d1-4cf7-9cba-153c0b84a738"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 15.49\n",
            "5 Most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
            "Number of rare words (appear once): 9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Sometimes useful for lemmatization\n",
        "\n",
        "# Then proceed with your code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrG6C-rP2x3h",
        "outputId": "706e5234-a46a-4300-8c4c-1d2adc83262e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Delete the cache and reinstall punkt\n",
        "nltk.data.path.append('/usr/local/share/nltk_data')  # Ensure correct path\n",
        "nltk.download('punkt', force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaJIsPzv3vFS",
        "outputId": "b283e937-d0cb-48b1-a068-80225446a746"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "# Set environment variable\n",
        "os.environ[\"NLTK_DATA\"] = \"/usr/local/share/nltk_data\"\n",
        "\n",
        "# Then try downloading again\n",
        "nltk.download('punkt', force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R989MJzU3zqA",
        "outputId": "23a94f2f-13ae-4526-b14f-2a77c6c0cfbd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import shutil\n",
        "\n",
        "# Remove previous NLTK data folder (Colab stores it under /root/nltk_data)\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "\n",
        "# Force fresh installation of necessary packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiraSJvN4IKE",
        "outputId": "1ef83b70-1223-4a28-d00e-dc58c9116afa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Ensure necessary downloads\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example processing\n",
        "text = str(df['message'][0])  # Ensure it's a string\n",
        "\n",
        "# Use RegexpTokenizer instead of word_tokenize()\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # This extracts only words, avoiding the punkt issue\n",
        "tokens_nltk = tokenizer.tokenize(text)\n",
        "\n",
        "lemmas_nltk = [lemmatizer.lemmatize(token) for token in tokens_nltk]\n",
        "stems_nltk = [stemmer.stem(token) for token in tokens_nltk]\n",
        "\n",
        "print(\"NLTK Tokenization (using RegexpTokenizer):\", tokens_nltk)\n",
        "print(\"NLTK Lemmatization:\", lemmas_nltk)\n",
        "print(\"NLTK Stemming:\", stems_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfKFzZBK2Iy0",
        "outputId": "e64c8c3c-1820-4341-e955-843142ecec81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokenization (using RegexpTokenizer): ['Go', 'until', 'jurong', 'point', 'crazy', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'Cine', 'there', 'got', 'amore', 'wat']\n",
            "NLTK Lemmatization: ['Go', 'until', 'jurong', 'point', 'crazy', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'Cine', 'there', 'got', 'amore', 'wat']\n",
            "NLTK Stemming: ['go', 'until', 'jurong', 'point', 'crazi', 'avail', 'onli', 'in', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amor', 'wat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example processing\n",
        "text = str(df['message'][0])  # Ensure it's a string\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract tokenization, lemmatization, and stemming (in SpaCy, lemmatization is used instead of stemming)\n",
        "tokens_spacy = [token.text for token in doc]\n",
        "lemmas_spacy = [token.lemma_ for token in doc]\n",
        "stems_spacy = [token.lemma_ for token in doc]  # SpaCy doesn't have separate stemming, so we use lemma_\n",
        "\n",
        "print(\"SpaCy Tokenization:\", tokens_spacy)\n",
        "print(\"SpaCy Lemmatization:\", lemmas_spacy)\n",
        "print(\"SpaCy 'Stemming' (same as Lemmatization in SpaCy):\", stems_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dk90qXi5zwe",
        "outputId": "3649d79d-32f3-474b-da48-22a4edb28afa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpaCy Tokenization: ['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n",
            "SpaCy Lemmatization: ['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...']\n",
            "SpaCy 'Stemming' (same as Lemmatization in SpaCy): ['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Create new columns in dataframe to store processed versions\n",
        "df['tokens_nltk'] = df['message'].apply(lambda x: RegexpTokenizer(r'\\w+').tokenize(str(x)))\n",
        "df['lemmas_nltk'] = df['tokens_nltk'].apply(lambda tokens: [WordNetLemmatizer().lemmatize(token) for token in tokens])\n",
        "df['stems_nltk'] = df['tokens_nltk'].apply(lambda tokens: [PorterStemmer().stem(token) for token in tokens])\n",
        "\n",
        "# Repeat for SpaCy\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_processing(text):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.text for token in doc]\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    stems = lemmas  # Stemming in SpaCy is handled by lemmas\n",
        "    return tokens, lemmas, stems\n",
        "\n",
        "df[['tokens_spacy', 'lemmas_spacy', 'stems_spacy']] = df['message'].apply(lambda x: pd.Series(spacy_processing(x)))\n",
        "\n",
        "# Compute Statistics for Each Version\n",
        "def compute_stats(column):\n",
        "    word_list = [word for message in df[column] for word in message]  # Flatten list\n",
        "    word_freq = Counter(word_list)\n",
        "\n",
        "    num_messages = len(df)\n",
        "    num_spam = len(df[df['label'] == 'spam'])\n",
        "    avg_words = sum(len(message) for message in df[column]) / num_messages\n",
        "    most_common_words = word_freq.most_common(5)\n",
        "    rare_words = sum(1 for word, count in word_freq.items() if count == 1)\n",
        "\n",
        "    return num_messages, num_spam, avg_words, most_common_words, rare_words\n",
        "\n",
        "# Print stats for tokenized, lemmatized, and stemmed versions\n",
        "for version in ['tokens_nltk', 'lemmas_nltk', 'stems_nltk', 'tokens_spacy', 'lemmas_spacy', 'stems_spacy']:\n",
        "    stats = compute_stats(version)\n",
        "    print(f\"\\nStatistics for {version}:\")\n",
        "    print(f\"Total messages: {stats[0]}\")\n",
        "    print(f\"Spam messages: {stats[1]}\")\n",
        "    print(f\"Average words per message: {stats[2]:.2f}\")\n",
        "    print(f\"5 Most frequent words: {stats[3]}\")\n",
        "    print(f\"Number of rare words (appear once): {stats[4]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkvzQ4kg7821",
        "outputId": "c86fd717-b09c-423b-dc3b-0f54fcde54a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistics for tokens_nltk:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 16.17\n",
            "5 Most frequent words: [('to', 2148), ('I', 2013), ('you', 1896), ('a', 1332), ('the', 1202)]\n",
            "Number of rare words (appear once): 5606\n",
            "\n",
            "Statistics for lemmas_nltk:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 16.17\n",
            "5 Most frequent words: [('to', 2148), ('I', 2013), ('you', 1896), ('a', 1470), ('the', 1202)]\n",
            "Number of rare words (appear once): 5322\n",
            "\n",
            "Statistics for stems_nltk:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 16.17\n",
            "5 Most frequent words: [('i', 3001), ('to', 2242), ('you', 2240), ('a', 1433), ('the', 1328)]\n",
            "Number of rare words (appear once): 3555\n",
            "\n",
            "Statistics for tokens_spacy:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 18.58\n",
            "5 Most frequent words: [('.', 4945), ('to', 2148), ('I', 1988), ('you', 1878), (',', 1857)]\n",
            "Number of rare words (appear once): 6272\n",
            "\n",
            "Statistics for lemmas_spacy:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 18.58\n",
            "5 Most frequent words: [('.', 4945), ('I', 3729), ('be', 3248), ('to', 2309), ('you', 2214)]\n",
            "Number of rare words (appear once): 5332\n",
            "\n",
            "Statistics for stems_spacy:\n",
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Average words per message: 18.58\n",
            "5 Most frequent words: [('.', 4945), ('I', 3729), ('be', 3248), ('to', 2309), ('you', 2214)]\n",
            "Number of rare words (appear once): 5332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Wikipedia page URL\n",
        "url = \"https://en.wikipedia.org/wiki/Duck\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse HTML content\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract page title\n",
        "title = soup.find(\"h1\").text\n",
        "print(f\"Page Title: {title}\")\n",
        "\n",
        "# Extract all headings (subsections)\n",
        "headings = [h.text for h in soup.find_all([\"h2\", \"h3\"])]\n",
        "print(\"\\nHeadings Found:\")\n",
        "print(\"\\n\".join(headings))\n",
        "\n",
        "# Extract all paragraph text\n",
        "paragraphs = [p.text for p in soup.find_all(\"p\")]\n",
        "\n",
        "# Print first 5 paragraphs\n",
        "print(\"\\nSample Paragraphs:\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nParagraph {i+1}:\\n{paragraphs[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqAUqmn9Ak-J",
        "outputId": "47cc865b-d6de-4a0e-f30e-f41ff7283820"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Duck\n",
            "\n",
            "Headings Found:\n",
            "Contents\n",
            "Etymology\n",
            "Taxonomy\n",
            "Morphology\n",
            "Distribution and habitat\n",
            "Behaviour\n",
            "Feeding\n",
            "Breeding\n",
            "Communication\n",
            "Predators\n",
            "Relationship with humans\n",
            "Hunting\n",
            "Domestication\n",
            "Heraldry\n",
            "Cultural references\n",
            "See also\n",
            "Notes\n",
            "Citations\n",
            "Sources\n",
            "External links\n",
            "\n",
            "Sample Paragraphs:\n",
            "\n",
            "Paragraph 1:\n",
            "\n",
            "\n",
            "\n",
            "Paragraph 2:\n",
            "See text\n",
            "\n",
            "\n",
            "Paragraph 3:\n",
            "Duck is the common name for numerous species of waterfowl in the family Anatidae. Ducks are generally smaller and shorter-necked than swans and geese, which are members of the same family. Divided among several subfamilies, they are a form taxon; they do not represent a monophyletic group (the group of all descendants of a single common ancestral species), since swans and geese are not considered ducks. Ducks are mostly aquatic birds, and may be found in both fresh water and sea water.\n",
            "\n",
            "\n",
            "Paragraph 4:\n",
            "Ducks are sometimes confused with several types of unrelated water birds with similar forms, such as loons or divers, grebes, gallinules and coots.\n",
            "\n",
            "\n",
            "Paragraph 5:\n",
            "The word duck comes from Old English dūce 'diver', a derivative of the verb *dūcan 'to duck, bend down low as if to get under something, or dive', because of the way many species in the dabbling duck group feed by upending; compare with Dutch duiken and German tauchen 'to dive'.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Ensure necessary downloads\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize NLTK tools\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Example processing for scraped text\n",
        "def process_text_nltk(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stems = [stemmer.stem(token) for token in tokens]\n",
        "    return tokens, lemmas, stems\n",
        "\n",
        "# Assuming 'paragraphs' contains the scraped Wikipedia text\n",
        "processed_nltk = [process_text_nltk(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "# Display sample results\n",
        "for i in range(3):  # Print first 3 processed paragraphs\n",
        "    print(f\"\\nParagraph {i+1} (NLTK):\")\n",
        "    print(f\"Tokens: {processed_nltk[i][0]}\")\n",
        "    print(f\"Lemmas: {processed_nltk[i][1]}\")\n",
        "    print(f\"Stems: {processed_nltk[i][2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6-R44SAAn5K",
        "outputId": "ca6553ec-b98d-492f-c2d5-6d597b17edb5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paragraph 1 (NLTK):\n",
            "Tokens: []\n",
            "Lemmas: []\n",
            "Stems: []\n",
            "\n",
            "Paragraph 2 (NLTK):\n",
            "Tokens: ['See', 'text']\n",
            "Lemmas: ['See', 'text']\n",
            "Stems: ['see', 'text']\n",
            "\n",
            "Paragraph 3 (NLTK):\n",
            "Tokens: ['Duck', 'is', 'the', 'common', 'name', 'for', 'numerous', 'species', 'of', 'waterfowl', 'in', 'the', 'family', 'Anatidae', 'Ducks', 'are', 'generally', 'smaller', 'and', 'shorter', 'necked', 'than', 'swans', 'and', 'geese', 'which', 'are', 'members', 'of', 'the', 'same', 'family', 'Divided', 'among', 'several', 'subfamilies', 'they', 'are', 'a', 'form', 'taxon', 'they', 'do', 'not', 'represent', 'a', 'monophyletic', 'group', 'the', 'group', 'of', 'all', 'descendants', 'of', 'a', 'single', 'common', 'ancestral', 'species', 'since', 'swans', 'and', 'geese', 'are', 'not', 'considered', 'ducks', 'Ducks', 'are', 'mostly', 'aquatic', 'birds', 'and', 'may', 'be', 'found', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water']\n",
            "Lemmas: ['Duck', 'is', 'the', 'common', 'name', 'for', 'numerous', 'specie', 'of', 'waterfowl', 'in', 'the', 'family', 'Anatidae', 'Ducks', 'are', 'generally', 'smaller', 'and', 'shorter', 'necked', 'than', 'swan', 'and', 'goose', 'which', 'are', 'member', 'of', 'the', 'same', 'family', 'Divided', 'among', 'several', 'subfamily', 'they', 'are', 'a', 'form', 'taxon', 'they', 'do', 'not', 'represent', 'a', 'monophyletic', 'group', 'the', 'group', 'of', 'all', 'descendant', 'of', 'a', 'single', 'common', 'ancestral', 'specie', 'since', 'swan', 'and', 'goose', 'are', 'not', 'considered', 'duck', 'Ducks', 'are', 'mostly', 'aquatic', 'bird', 'and', 'may', 'be', 'found', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water']\n",
            "Stems: ['duck', 'is', 'the', 'common', 'name', 'for', 'numer', 'speci', 'of', 'waterfowl', 'in', 'the', 'famili', 'anatida', 'duck', 'are', 'gener', 'smaller', 'and', 'shorter', 'neck', 'than', 'swan', 'and', 'gees', 'which', 'are', 'member', 'of', 'the', 'same', 'famili', 'divid', 'among', 'sever', 'subfamili', 'they', 'are', 'a', 'form', 'taxon', 'they', 'do', 'not', 'repres', 'a', 'monophylet', 'group', 'the', 'group', 'of', 'all', 'descend', 'of', 'a', 'singl', 'common', 'ancestr', 'speci', 'sinc', 'swan', 'and', 'gees', 'are', 'not', 'consid', 'duck', 'duck', 'are', 'mostli', 'aquat', 'bird', 'and', 'may', 'be', 'found', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example processing for scraped text\n",
        "def process_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    stems = lemmas  # Since SpaCy uses lemmatization instead of traditional stemming\n",
        "    return tokens, lemmas, stems\n",
        "\n",
        "# Assuming 'paragraphs' contains the scraped Wikipedia text\n",
        "processed_spacy = [process_text_spacy(paragraph) for paragraph in paragraphs]\n",
        "\n",
        "# Display sample results\n",
        "for i in range(3):  # Print first 3 processed paragraphs\n",
        "    print(f\"\\nParagraph {i+1} (SpaCy):\")\n",
        "    print(f\"Tokens: {processed_spacy[i][0]}\")\n",
        "    print(f\"Lemmas: {processed_spacy[i][1]}\")\n",
        "    print(f\"Stems (same as Lemmas in SpaCy): {processed_spacy[i][2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFrBwJqpBdhK",
        "outputId": "5765cfb8-c860-4895-f298-c5833ebfb234"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paragraph 1 (SpaCy):\n",
            "Tokens: ['\\n']\n",
            "Lemmas: ['\\n']\n",
            "Stems (same as Lemmas in SpaCy): ['\\n']\n",
            "\n",
            "Paragraph 2 (SpaCy):\n",
            "Tokens: ['See', 'text', '\\n']\n",
            "Lemmas: ['see', 'text', '\\n']\n",
            "Stems (same as Lemmas in SpaCy): ['see', 'text', '\\n']\n",
            "\n",
            "Paragraph 3 (SpaCy):\n",
            "Tokens: ['Duck', 'is', 'the', 'common', 'name', 'for', 'numerous', 'species', 'of', 'waterfowl', 'in', 'the', 'family', 'Anatidae', '.', 'Ducks', 'are', 'generally', 'smaller', 'and', 'shorter', '-', 'necked', 'than', 'swans', 'and', 'geese', ',', 'which', 'are', 'members', 'of', 'the', 'same', 'family', '.', 'Divided', 'among', 'several', 'subfamilies', ',', 'they', 'are', 'a', 'form', 'taxon', ';', 'they', 'do', 'not', 'represent', 'a', 'monophyletic', 'group', '(', 'the', 'group', 'of', 'all', 'descendants', 'of', 'a', 'single', 'common', 'ancestral', 'species', ')', ',', 'since', 'swans', 'and', 'geese', 'are', 'not', 'considered', 'ducks', '.', 'Ducks', 'are', 'mostly', 'aquatic', 'birds', ',', 'and', 'may', 'be', 'found', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water', '.', '\\n']\n",
            "Lemmas: ['duck', 'be', 'the', 'common', 'name', 'for', 'numerous', 'specie', 'of', 'waterfowl', 'in', 'the', 'family', 'Anatidae', '.', 'duck', 'be', 'generally', 'small', 'and', 'shorter', '-', 'neck', 'than', 'swan', 'and', 'goose', ',', 'which', 'be', 'member', 'of', 'the', 'same', 'family', '.', 'divide', 'among', 'several', 'subfamily', ',', 'they', 'be', 'a', 'form', 'taxon', ';', 'they', 'do', 'not', 'represent', 'a', 'monophyletic', 'group', '(', 'the', 'group', 'of', 'all', 'descendant', 'of', 'a', 'single', 'common', 'ancestral', 'specie', ')', ',', 'since', 'swan', 'and', 'goose', 'be', 'not', 'consider', 'duck', '.', 'duck', 'be', 'mostly', 'aquatic', 'bird', ',', 'and', 'may', 'be', 'find', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water', '.', '\\n']\n",
            "Stems (same as Lemmas in SpaCy): ['duck', 'be', 'the', 'common', 'name', 'for', 'numerous', 'specie', 'of', 'waterfowl', 'in', 'the', 'family', 'Anatidae', '.', 'duck', 'be', 'generally', 'small', 'and', 'shorter', '-', 'neck', 'than', 'swan', 'and', 'goose', ',', 'which', 'be', 'member', 'of', 'the', 'same', 'family', '.', 'divide', 'among', 'several', 'subfamily', ',', 'they', 'be', 'a', 'form', 'taxon', ';', 'they', 'do', 'not', 'represent', 'a', 'monophyletic', 'group', '(', 'the', 'group', 'of', 'all', 'descendant', 'of', 'a', 'single', 'common', 'ancestral', 'specie', ')', ',', 'since', 'swan', 'and', 'goose', 'be', 'not', 'consider', 'duck', '.', 'duck', 'be', 'mostly', 'aquatic', 'bird', ',', 'and', 'may', 'be', 'find', 'in', 'both', 'fresh', 'water', 'and', 'sea', 'water', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file manually from local machine\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load text from file (assuming filename is 'WhatsApp Chat.txt')\n",
        "file_path = \"WhatsApp Chat.txt\"\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    chat_text = file.read()\n",
        "\n",
        "print(\"Sample WhatsApp chat text:\\n\", chat_text[:500])  # Display first 500 characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "YSZDSI4HFJrt",
        "outputId": "f1aaaa94-6d40-458e-d512-b3f20888ba4f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0a22ae6e-135c-47cf-9169-fc32138f69ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0a22ae6e-135c-47cf-9169-fc32138f69ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Chat.txt to WhatsApp Chat.txt\n",
            "Sample WhatsApp chat text:\n",
            " 13/08/2020, 08:19 - Messages and calls are end-to-end encrypted. Only people in this chat can read, listen to, or share them. Learn more.\n",
            "13/08/2020, 08:19 - ‎‫אמא‬‎ created group \"‎‫כל מה שצריך לקנות‬‎\"\n",
            "13/08/2020, 08:19 - ‎‫אמא‬‎ added you\n",
            "13/08/2020, 08:19 - אמא: 2 סוכר לבן\n",
            "1 סוכר דמררה\n",
            "1 מפיות\n",
            "13/08/2020, 08:19 - אמא: קפה אדום\n",
            "13/08/2020, 08:20 - אמא: ביצים\n",
            "13/08/2020, 08:26 - אמא: סירופ מייפל\n",
            "13/08/2020, 13:44 - אם יורשה לי: שמנת מתוקה\n",
            "13/08/2020, 13:45 - משה: חציל\n",
            "13/08/2020, 13:47 - Danie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Ensure necessary downloads\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize tokenization\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Hebrew stemming function (basic heuristic approach)\n",
        "def simple_hebrew_stemmer(word):\n",
        "    suffixes = [\"ים\", \"ות\", \"ה\", \"ים\", \"ות\", \"י\", \"ת\"]\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "# Processing function\n",
        "def process_text_nltk(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    stems = [simple_hebrew_stemmer(token) for token in tokens]  # Apply heuristic stemming\n",
        "    return tokens, stems  # No official NLTK lemmatization for Hebrew\n",
        "\n",
        "tokens_nltk, stems_nltk = process_text_nltk(chat_text)\n",
        "\n",
        "print(\"\\nNLTK Tokenization (Hebrew):\", tokens_nltk[:50])\n",
        "print(\"NLTK Stemming (Hebrew - heuristic):\", stems_nltk[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xae1Xa7FFYwt",
        "outputId": "9a96e127-b984-423a-957d-d6d9a8d6c31d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Tokenization (Hebrew): ['13', '08', '2020', '08', '19', 'Messages', 'and', 'calls', 'are', 'end', 'to', 'end', 'encrypted', 'Only', 'people', 'in', 'this', 'chat', 'can', 'read', 'listen', 'to', 'or', 'share', 'them', 'Learn', 'more', '13', '08', '2020', '08', '19', 'אמא', 'created', 'group', 'כל', 'מה', 'שצריך', 'לקנות', '13', '08', '2020', '08', '19', 'אמא', 'added', 'you', '13', '08', '2020']\n",
            "NLTK Stemming (Hebrew - heuristic): ['13', '08', '2020', '08', '19', 'Messages', 'and', 'calls', 'are', 'end', 'to', 'end', 'encrypted', 'Only', 'people', 'in', 'this', 'chat', 'can', 'read', 'listen', 'to', 'or', 'share', 'them', 'Learn', 'more', '13', '08', '2020', '08', '19', 'אמא', 'created', 'group', 'כל', 'מ', 'שצריך', 'לקנ', '13', '08', '2020', '08', '19', 'אמא', 'added', 'you', '13', '08', '2020']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load a blank Hebrew tokenizer (since we can't install the full model)\n",
        "nlp = spacy.blank(\"he\")  # Creates a SpaCy pipeline for Hebrew without pre-trained components\n",
        "\n",
        "# Processing function for tokenization only (lemmatization won't work without a full model)\n",
        "def process_text_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    lemmas = tokens  # Since no Hebrew lemmatizer is available, we'll keep tokens as lemmas\n",
        "    stems = tokens   # Stemming also isn't supported in SpaCy for Hebrew, so we use tokens\n",
        "\n",
        "    return tokens, lemmas, stems\n",
        "\n",
        "# Process the Hebrew WhatsApp chat\n",
        "tokens_spacy, lemmas_spacy, stems_spacy = process_text_spacy(chat_text)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSpaCy Tokenization (Hebrew):\", tokens_spacy[:50])\n",
        "print(\"SpaCy Lemmatization (Fallback to Tokens):\", lemmas_spacy[:50])\n",
        "print(\"SpaCy 'Stemming' (Fallback to Tokens):\", stems_spacy[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEl40zgSGSWG",
        "outputId": "d7c1132b-36b1-48e9-e1ff-8729c58fbb2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SpaCy Tokenization (Hebrew): ['13/08/2020', ',', '08:19', '-', 'Messages', 'and', 'calls', 'are', 'end', '-', 'to', '-', 'end', 'encrypted', '.', 'Only', 'people', 'in', 'this', 'chat', 'can', 'read', ',', 'listen', 'to', ',', 'or', 'share', 'them', '.', 'Learn', 'more', '.', '\\n', '13/08/2020', ',', '08:19', '-', '\\u200e\\u202bאמא\\u202c\\u200e', 'created', 'group', '\"', '\\u200e\\u202bכל', 'מה', 'שצריך', 'לקנות\\u202c\\u200e', '\"', '\\n', '13/08/2020', ',']\n",
            "SpaCy Lemmatization (Fallback to Tokens): ['13/08/2020', ',', '08:19', '-', 'Messages', 'and', 'calls', 'are', 'end', '-', 'to', '-', 'end', 'encrypted', '.', 'Only', 'people', 'in', 'this', 'chat', 'can', 'read', ',', 'listen', 'to', ',', 'or', 'share', 'them', '.', 'Learn', 'more', '.', '\\n', '13/08/2020', ',', '08:19', '-', '\\u200e\\u202bאמא\\u202c\\u200e', 'created', 'group', '\"', '\\u200e\\u202bכל', 'מה', 'שצריך', 'לקנות\\u202c\\u200e', '\"', '\\n', '13/08/2020', ',']\n",
            "SpaCy 'Stemming' (Fallback to Tokens): ['13/08/2020', ',', '08:19', '-', 'Messages', 'and', 'calls', 'are', 'end', '-', 'to', '-', 'end', 'encrypted', '.', 'Only', 'people', 'in', 'this', 'chat', 'can', 'read', ',', 'listen', 'to', ',', 'or', 'share', 'them', '.', 'Learn', 'more', '.', '\\n', '13/08/2020', ',', '08:19', '-', '\\u200e\\u202bאמא\\u202c\\u200e', 'created', 'group', '\"', '\\u200e\\u202bכל', 'מה', 'שצריך', 'לקנות\\u202c\\u200e', '\"', '\\n', '13/08/2020', ',']\n"
          ]
        }
      ]
    }
  ]
}